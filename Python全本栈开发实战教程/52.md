# 用于方便查看回顾
# http://securitytech.cc/ 的免费文本教程

# [官网](securitytech.cc) 会长期更新工具、产品开发、产品开发教程、商业产品开发。视频教程。

# [本人介绍](http://securitytech.cc/about)

![公众号](https://github.com/haidragon/haidragon/blob/main/gzh.png)


 ---

## 日志系统集成：Python 标准 `logging` 模块，Sentry 错误监控，ELK 日志收集与分析

在任何一个生产级别的应用中，**日志 (Logging)** 都是至关重要的。它不仅能帮助我们**调试问题**，还能用于**监控应用运行状态、分析用户行为、识别潜在的安全威胁**等。一个完善的日志系统能够让我们在应用出现问题时快速定位并解决，从而确保服务的稳定性和用户体验。

本实践将涵盖 Python Web 应用中日志集成的三个主要方面：

1.  **Python 标准 `logging` 模块**：Python 内置的强大日志工具，是所有日志实践的基础。
2.  **Sentry 错误监控**：专注于捕获和聚合应用中的运行时错误和异常，提供实时警报和详细的错误上下文信息。
3.  **ELK Stack (Elasticsearch, Logstash, Kibana)**：一个强大的日志收集、存储、分析和可视化平台，适用于处理大规模日志数据。

---

### 一、Python 标准 `logging` 模块

Python 的 `logging` 模块是一个高度灵活的框架，用于发出日志消息。它支持不同的日志级别、多种处理器 (handlers) 将日志输出到文件、控制台、网络等，以及格式化器 (formatters) 控制日志的显示格式。

#### 1. 核心概念

* **Logger (记录器)**：这是你与日志系统交互的主要接口。你可以通过名称获取一个 Logger 实例。
* **Handler (处理器)**：负责将日志消息发送到目的地（如控制台、文件、网络、邮件等）。常见的有 `StreamHandler` (输出到控制台)、`FileHandler` (输出到文件)、`RotatingFileHandler` (按大小或时间轮转文件)。
* **Formatter (格式化器)**：定义日志消息的输出格式。
* **Level (日志级别)**：定义日志消息的严重性，从低到高依次为：
    * `DEBUG`：详细的调试信息，仅在开发阶段使用。
    * `INFO`：确认程序按预期运行。
    * `WARNING`：表示一个意外事件，或将来某个问题，但程序仍在正常工作。
    * `ERROR`：由于某个更严重的问题，程序无法执行某些功能。
    * `CRITICAL`：严重错误，表示程序本身可能无法继续运行。

#### 2. 基本使用

```python
import logging

# 1. 获取一个 Logger 实例 (通常以模块名命名)
logger = logging.getLogger(__name__)

# 2. 配置日志级别 (如果未配置，默认是 WARNING)
logger.setLevel(logging.DEBUG) # 设置为 DEBUG 级别，会记录所有低于或等于 DEBUG 的消息

# 3. 创建一个处理器 (例如，输出到控制台)
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO) # 控制台只显示 INFO 及以上级别的日志

# 4. 定义日志格式
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
console_handler.setFormatter(formatter)

# 5. 将处理器添加到 Logger
logger.addHandler(console_handler)

# 6. 发出日志消息
logger.debug("This is a debug message.")
logger.info("This is an info message.")
logger.warning("This is a warning message.")
logger.error("This is an error message.")
logger.critical("This is a critical message.")

# 也可以直接使用 logging 模块的根记录器
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
# logging.info("This is an info message from root logger.")
```

#### 3. 配置到 Web 框架

Python Web 框架通常会集成 `logging` 模块，并允许你在其配置文件中进行详细设置。

##### a. Django 日志配置

Django 的日志配置在 `settings.py` 文件中通过 `LOGGING` 字典进行。

**`your_django_project/settings.py`**

```python
# settings.py

LOGGING = {
    'version': 1,
    'disable_existing_loggers': False, # 保持 Django 默认日志器，不禁用

    'formatters': { # 定义日志格式
        'verbose': {
            'format': '{levelname} {asctime} {module} {process:d} {thread:d} {message}',
            'style': '{',
        },
        'simple': {
            'format': '{levelname} {asctime} {name} {message}',
            'style': '{',
        },
    },

    'handlers': { # 定义处理器
        'console': { # 输出到控制台
            'class': 'logging.StreamHandler',
            'formatter': 'simple',
            'level': 'INFO', # 开发环境通常只看 INFO 级别
        },
        'file': { # 输出到文件，按大小轮转
            'class': 'logging.handlers.RotatingFileHandler',
            'filename': '/var/log/django/debug.log', # 日志文件路径，生产环境确保可写
            'maxBytes': 1024 * 1024 * 5, # 5 MB
            'backupCount': 5, # 保留 5 个备份文件
            'formatter': 'verbose',
            'level': 'DEBUG', # 文件日志通常记录更详细的 DEBUG 信息
        },
        'error_file': { # 专门记录 ERROR 和 CRITICAL 级别错误的文件
            'class': 'logging.handlers.RotatingFileHandler',
            'filename': '/var/log/django/error.log',
            'maxBytes': 1024 * 1024 * 10, # 10 MB
            'backupCount': 3,
            'formatter': 'verbose',
            'level': 'ERROR',
        },
        'mail_admins': { # 错误邮件通知
            'class': 'django.utils.log.AdminEmailHandler',
            'level': 'ERROR',
            'include_html': True,
        }
    },

    'loggers': { # 定义记录器
        'django': { # Django 自身的日志
            'handlers': ['console', 'error_file'],
            'level': 'INFO',
            'propagate': False, # 不向上传播，避免重复打印
        },
        'django.request': { # Django 请求相关的日志
            'handlers': ['error_file', 'mail_admins'],
            'level': 'ERROR',
            'propagate': False,
        },
        'my_app': { # 你的应用模块的日志
            'handlers': ['console', 'file', 'error_file'],
            'level': 'DEBUG', # 你的应用可以记录 DEBUG 级别
            'propagate': False,
        },
        '': { # 根记录器，捕获所有未被特定记录器处理的日志
            'handlers': ['console', 'file'],
            'level': 'INFO',
        }
    }
}

# 确保在生产环境设置 ADMINS 和 SERVER_EMAIL 以接收错误邮件
# ADMINS = [('Your Name', 'your_email@example.com')]
# SERVER_EMAIL = 'webmaster@yourdomain.com'
```

**在 Django 应用中使用日志：**

```python
# my_app/views.py
import logging

logger = logging.getLogger(__name__) # 获取当前模块的记录器实例

def my_view(request):
    logger.info("User accessed my_view.")
    try:
        # 模拟一个错误
        result = 1 / 0
    except ZeroDivisionError:
        logger.error("A ZeroDivisionError occurred in my_view!", exc_info=True) # exc_info=True 会将异常堆栈信息记录到日志中
        return JsonResponse({"error": "An internal error occurred."}, status=500)
    return JsonResponse({"message": "Hello from Django."})
```

##### b. FastAPI 日志配置

FastAPI 通常与 Uvicorn 一起使用，Uvicorn 有自己的日志配置。你也可以通过 Python 的 `logging` 模块进行更精细的控制。

**`your_fastapi_project/main.py`**

```python
import logging
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse
import uvicorn

# 1. 获取一个 Logger 实例
logger = logging.getLogger(__name__)

# 2. 配置 Logger (可以在应用启动时进行，或者在 uvicorn.run 中配置 log_config)
# 推荐使用 uvicorn 的 log_config 文件，或者在 main.py 中直接配置
# 这里示范直接配置，更灵活
logger.setLevel(logging.INFO) # 默认级别

# 创建一个 StreamHandler 输出到控制台
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

# 也可以添加 FileHandler
file_handler = logging.handlers.RotatingFileHandler(
    '/var/log/fastapi/app.log', # 生产环境确保可写
    maxBytes=1024 * 1024 * 5, # 5 MB
    backupCount=5
)
file_handler.setLevel(logging.DEBUG)
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)


app = FastAPI(title="FastAPI 日志 Demo")

@app.get("/")
async def read_root(request: Request):
    logger.info(f"Root endpoint accessed by {request.client.host}")
    return {"message": "Hello from FastAPI."}

@app.get("/divide/{a}/{b}")
async def divide_numbers(a: int, b: int):
    logger.debug(f"Attempting to divide {a} by {b}")
    try:
        result = a / b
        logger.info(f"Division successful: {a} / {b} = {result}")
        return {"result": result}
    except ZeroDivisionError as e:
        logger.error(f"Division by zero attempted: {a} / {b}. Error: {e}", exc_info=True)
        return JSONResponse(status_code=400, content={"error": "Cannot divide by zero!"})
    except Exception as e:
        logger.exception(f"An unexpected error occurred during division: {e}") # logging.exception 会自动添加 exc_info=True
        return JSONResponse(status_code=500, content={"error": "An unexpected error occurred."})

if __name__ == "__main__":
    # Uvicorn 可以通过 log_config 参数加载日志配置
    # 默认情况下，Uvicorn 已经处理了控制台输出，这里只是演示更细致的自定义
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        # log_config=None # 如果你完全自定义了日志，可以禁用 uvicorn 的默认日志配置
    )
```

##### c. Flask 日志配置

Flask 应用可以通过 Python 标准 `logging` 模块配置。

**`your_flask_project/app.py`**

```python
import logging
from flask import Flask, jsonify, request
from logging.handlers import RotatingFileHandler

app = Flask(__name__)

# 1. 配置 Flask 应用的日志
if not app.debug: # 仅在非调试模式下配置生产级日志
    # 设置日志级别
    app.logger.setLevel(logging.INFO)

    # 创建一个文件处理器
    file_handler = RotatingFileHandler(
        '/var/log/flask/app.log', # 生产环境确保可写
        maxBytes=1024 * 1024 * 5, # 5 MB
        backupCount=5
    )
    # 设置日志级别
    file_handler.setLevel(logging.DEBUG)
    # 定义日志格式
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)

    # 将处理器添加到 Flask 应用的 logger
    app.logger.addHandler(file_handler)

    # 也可以添加控制台处理器 (在生产环境可能不需要，因为由 Gunicorn/Nginx 收集)
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    app.logger.addHandler(console_handler)

@app.route("/")
def index():
    app.logger.info(f"Root endpoint accessed by {request.remote_addr}")
    return jsonify({"message": "Hello from Flask."})

@app.route("/calculate")
def calculate():
    try:
        num1 = int(request.args.get('n1'))
        num2 = int(request.args.get('n2'))
        app.logger.debug(f"Calculating {num1} / {num2}")
        result = num1 / num2
        app.logger.info(f"Calculation successful: {num1} / {num2} = {result}")
        return jsonify({"result": result})
    except (ValueError, TypeError):
        app.logger.warning("Invalid input for calculation.")
        return jsonify({"error": "Invalid input. Please provide valid numbers."}), 400
    except ZeroDivisionError:
        app.logger.error("Division by zero attempt!", exc_info=True)
        return jsonify({"error": "Cannot divide by zero!"}), 400
    except Exception as e:
        app.logger.exception(f"An unexpected error occurred: {e}")
        return jsonify({"error": "An unexpected error occurred."}), 500

if __name__ == "__main__":
    app.run(debug=True, port=5000)
```

---

### 二、Sentry 错误监控

**Sentry** 是一个实时错误监控和聚合平台。它专注于捕获应用中的**未捕获异常、错误、性能问题**等，并提供丰富的上下文信息（如堆栈跟踪、请求参数、用户信息、设备信息等），帮助开发者快速定位和修复问题。

Sentry 提供云服务，也可以自托管。

#### 1. Sentry 优势

* **实时告警**：当错误发生时立即通过邮件、Slack 等方式通知。
* **详细上下文**：自动收集错误发生时的环境信息，包括堆栈跟踪、请求数据、用户数据、本地变量等。
* **错误聚合**：将相似的错误自动分组，减少通知噪音。
* **性能监控 (APM)**：提供分布式跟踪和性能指标，帮助识别性能瓶颈。
* **集成**：支持与 GitHub、Jira、Slack 等常用工具集成。

#### 2. 集成 Sentry

**准备工作：**

1.  在 Sentry 官网 (sentry.io) 注册账号，创建一个新项目，并获取项目的 **DSN (Data Source Name)**。DSN 是连接 Sentry 服务的唯一标识。
2.  安装 Sentry SDK for Python：`pip install sentry-sdk`

##### a. Django 集成 Sentry

Sentry SDK 提供了专门的 Django 集成。

**`your_django_project/settings.py`**

```python
# settings.py
import sentry_sdk
from sentry_sdk.integrations.django import DjangoIntegration

sentry_sdk.init(
    dsn="YOUR_SENTRY_DSN", # 替换为你的 DSN
    integrations=[
        DjangoIntegration(),
    ],

    # 设置采样率 (0.0 - 1.0)，控制发送到 Sentry 的事件数量
    # 生产环境通常不设置为 1.0，以避免数据量过大
    traces_sample_rate=1.0, # 性能监控采样率 (APM)
    profiles_sample_rate=1.0, # Profiling 采样率

    # 捕获所有错误，包括 4xx HTTP 错误
    send_default_pii=True, # 发送默认的个人身份信息（如IP地址），谨慎使用
    # debug=True, # 调试模式，在开发环境可以开启查看 Sentry SDK 输出
)

# ... 其他 Django settings ...
```

**在 Django 中手动捕获错误 (如果需要)：**

```python
# my_app/views.py
import sentry_sdk # 导入 sentry_sdk
from django.http import JsonResponse

def risky_view(request):
    try:
        result = 1 / 0
    except ZeroDivisionError as e:
        # 手动捕获并发送错误到 Sentry
        sentry_sdk.capture_exception(e)
        return JsonResponse({"error": "A critical error occurred."}, status=500)
    return JsonResponse({"message": "Success"})
```

##### b. FastAPI 集成 Sentry

Sentry SDK 提供了 ASGI 集成，适用于 FastAPI。

**`your_fastapi_project/main.py`**

```python
import sentry_sdk
from sentry_sdk.integrations.asgi import SentryAsgiMiddleware # ASGI 集成
from fastapi import FastAPI, HTTPException
from fastapi.responses import JSONResponse

sentry_sdk.init(
    dsn="YOUR_SENTRY_DSN", # 替换为你的 DSN
    integrations=[
        SentryAsgiMiddleware, # FastAPI 是 ASGI 应用，使用这个集成
    ],
    traces_sample_rate=1.0,
    profiles_sample_rate=1.0,
    # debug=True,
)

app = FastAPI(title="FastAPI Sentry Demo")

# 将 Sentry ASGI 中间件添加到 FastAPI 应用
app.add_middleware(SentryAsgiMiddleware)

@app.get("/")
async def read_root():
    return {"message": "Hello from FastAPI with Sentry."}

@app.get("/cause_error/")
async def cause_error():
    # 模拟一个未捕获的异常
    raise ValueError("This is an intentional error from FastAPI!")

@app.get("/capture_error/")
async def capture_error():
    try:
        # 模拟一个捕获的异常，并手动发送到 Sentry
        result = 1 / 0
    except ZeroDivisionError as e:
        sentry_sdk.capture_exception(e)
        return JSONResponse(status_code=500, content={"message": "Error captured and reported."})
    return {"message": "Success."}
```

##### c. Flask 集成 Sentry

Sentry SDK 提供了 Flask 集成。

**`your_flask_project/app.py`**

```python
import sentry_sdk
from sentry_sdk.integrations.flask import FlaskIntegration # Flask 集成
from flask import Flask, jsonify, request

sentry_sdk.init(
    dsn="YOUR_SENTRY_DSN", # 替换为你的 DSN
    integrations=[
        FlaskIntegration(),
    ],
    traces_sample_rate=1.0,
    profiles_sample_rate=1.0,
    # debug=True,
)

app = Flask(__name__)

@app.route("/")
def index():
    return jsonify({"message": "Hello from Flask with Sentry."})

@app.route("/cause_error")
def cause_error_flask():
    # 模拟一个未捕获的异常
    raise TypeError("This is an intentional error from Flask!")

@app.route("/capture_error_flask")
def capture_error_flask():
    try:
        # 模拟一个捕获的异常，并手动发送到 Sentry
        result = [1, 2, 3][5] # 索引越界
    except IndexError as e:
        sentry_sdk.capture_exception(e)
        return jsonify({"message": "Error captured and reported."}), 500
    return jsonify({"message": "Success."})

if __name__ == "__main__":
    app.run(debug=True, port=5000)
```

---

### 三、ELK Stack (Elasticsearch, Logstash, Kibana) 日志收集与分析

ELK Stack 是一个强大的开源解决方案，用于**日志收集、处理、存储和可视化**。它非常适合处理大量的、多样化的应用日志。

* **Elasticsearch**：一个分布式、实时的搜索和分析引擎，用于存储和索引日志数据。
* **Logstash**：一个数据收集引擎，从各种源（文件、数据库、网络等）收集数据，进行处理（解析、过滤、转换），然后发送到 Elasticsearch。
* **Kibana**：一个用于 Elasticsearch 的数据可视化工具，提供图表、仪表盘等功能，方便用户探索和分析日志数据。

#### 1. ELK 架构概览

客户端应用 (`logging` 输出到文件/控制台) -> 日志收集器 (Filebeat/Logstash) -> Logstash (数据处理) -> Elasticsearch (数据存储和索引) -> Kibana (数据可视化)

#### 2. 日志收集策略

由于 ELK 本身是一个复杂的独立系统，这里我们重点关注如何将 Python 应用的日志发送到 ELK，主要有两种常见方式：

##### 方式 A: 文件日志 + Filebeat (推荐)

这是最常用和推荐的方式。Python 应用将日志输出到本地文件，然后由轻量级的 **Filebeat** 负责监控这些日志文件，并将其转发给 Logstash 或直接发送到 Elasticsearch。

* **优点**：对应用侵入性小，应用只需关注日志写入文件；Filebeat 资源消耗低，可靠性高。
* **缺点**：日志在到达 ELK 前需要经过磁盘存储，实时性略逊于直接发送。

**配置步骤：**

1.  **Python 应用配置**：
    确保你的 Django/FastAPI/Flask 应用已经配置了 `logging.handlers.RotatingFileHandler` 或类似的 **文件处理器**，将日志输出到指定的文件路径（例如 `/var/log/your_app/app.log`）。

    ```python
    # 示例 (Django settings.py 或其他框架的日志配置)
    'file': {
        'class': 'logging.handlers.RotatingFileHandler',
        'filename': '/var/log/my_python_app/app.log', # 日志文件路径
        'maxBytes': 1024 * 1024 * 5,
        'backupCount': 5,
        'formatter': 'verbose',
        'level': 'DEBUG',
    },
    # ... 在 loggers 中使用这个 handler
    ```
2.  **安装和配置 Filebeat**：
    在运行 Python 应用的服务器上安装 Filebeat。

    **`filebeat.yml` (配置示例)**

    ```yaml
    # filebeat.yml

    filebeat.inputs:
    - type: log
      enabled: true
      paths:
        - /var/log/my_python_app/*.log # 监控你的应用日志文件
      fields:
        app_name: my_python_app # 自定义字段，用于标识日志来源应用
        environment: production # 环境标识

    output.logstash: # 将日志发送到 Logstash
      hosts: ["your_logstash_host:5044"] # 替换为你的 Logstash 服务器地址和端口

    # 或者直接发送到 Elasticsearch (如果不需要 Logstash 进行复杂处理)
    # output.elasticsearch:
    #   hosts: ["your_elasticsearch_host:9200"]
    #   username: "elastic"
    #   password: "changeme"
    ```
3.  **配置 Logstash (如果使用)**：
    Logstash 负责接收 Filebeat 的数据，解析、过滤、丰富日志，然后发送到 Elasticsearch。

    **`logstash.conf` (输入/过滤/输出配置示例)**

    ```conf
    # logstash.conf

    input {
      beats { # 接收 Filebeat 的输入
        port => 5044 # 监听 5044 端口
      }
    }

    filter {
      # JSON 格式日志解析
      json {
        source => "message" # 如果你的日志是 JSON 字符串，则使用此配置
        target => "json_data"
      }
      # 如果日志是普通文本，可以使用 grok 解析
      # grok {
      #   match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:loglevel} %{DATA:logger_name} %{GREEDYDATA:log_message}" }
      # }

      # 添加一些通用字段
      mutate {
        add_field => { "[@metadata][app_name]" => "%{[fields][app_name]}" }
        add_field => { "[@metadata][environment]" => "%{[fields][environment]}" }
      }
    }

    output {
      elasticsearch {
        hosts => ["your_elasticsearch_host:9200"] # 替换为你的 Elasticsearch 地址
        user => "elastic"
        password => "changeme"
        index => "my_python_app-%{+YYYY.MM.dd}" # 索引名称，按日期命名
      }
      # stdout { codec => rubydebug } # 调试时输出到控制台
    }
    ```
4.  **运行 Elasticsearch 和 Kibana**：
    根据 Elasticsearch 和 Kibana 的官方文档进行安装和配置。
5.  **在 Kibana 中可视化**：
    登录 Kibana，创建一个索引模式 (`my_python_app-*`)，然后你就可以在 `Discover` 页面查看日志，并在 `Visualize` 和 `Dashboard` 页面创建各种图表和仪表盘来分析日志数据。

##### 方式 B: 直接发送到 Logstash / Elasticsearch (不推荐直接用于生产，除非使用特定库)

这种方式是 Python 应用直接作为客户端，通过网络将日志发送到 Logstash 或 Elasticsearch。

* **优点**：实时性最高。
* **缺点**：
    * 增加应用本身的复杂性，需要处理网络连接、错误重试等。
    * 如果 Logstash/Elasticsearch 不可用，可能会阻塞应用。
    * 需要额外的库（如 `python-logstash` 或 `elasticsearch-py`）。
    * Filebeat 通常是更可靠和高效的选择。

**示例 (使用 `python-logstash` 库发送到 Logstash):**

```python
# pip install python-logstash
import logging
import logstash
import sys

# 假设 Logstash 运行在 localhost:5959
host = 'localhost'
port = 5959 # Logstash GELF input 默认端口，或者 5000 (TCP input)

logger = logging.getLogger('my_logstash_app')
logger.setLevel(logging.INFO)

# 添加 LogstashHandler
handler = logstash.LogstashHandler(host, port, version=1) # version=1 是 Logstash v1 格式
logger.addHandler(handler)

# 也可以将控制台输出保留
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

logger.info('This is a test message from Python to Logstash.')
logger.error('This is an error message.', extra={'tags': ['python', 'test'], 'project': 'my_web_app'})
try:
    raise ValueError("Something went wrong!")
except ValueError:
    logger.exception('An exception occurred.') # exception() 会自动发送异常信息
```
**注意：** 如果使用这种方式，你需要在 Logstash 中配置相应的输入插件 (例如 `tcp` 或 `udp`)。

```conf
# logstash.conf (input 示例)
input {
  tcp {
    port => 5959
    codec => json_lines # 如果 Python 发送的是 JSON 行
    # 或者如果你使用 python-logstash 库，它通常使用自己的协议，所以需要相应的 input plugin
    # beats { port => 5044 } # 如果是 beats 协议
  }
}
```

---

### 四、总结与最佳实践

一个健壮的日志系统是应用可靠运行的基石。

1.  **Python `logging` 模块是基础**：
    * 始终使用 `logging` 模块进行日志记录。
    * 根据不同的日志级别区分信息，如 `DEBUG` 用于开发调试，`INFO` 用于记录关键业务流程，`WARNING` 用于潜在问题，`ERROR` 和 `CRITICAL` 用于异常和系统故障。
    * 利用 `RotatingFileHandler` 进行文件轮转，避免日志文件过大。
    * 在生产环境，将日志输出到文件是最佳实践，而不是直接输出到控制台。控制台日志可以被 Docker 或进程管理器收集，但文件日志更灵活。

2.  **Sentry 用于错误监控和告警**：
    * **所有生产应用都应该集成 Sentry**。它能第一时间告诉你应用何时、何地、为何出错了，并提供丰富上下文，大大缩短排查时间。
    * 将 `exc_info=True` 或 `logger.exception()` 用于记录异常，Sentry 会自动捕获这些信息。
    * 配置合理的采样率，避免 Sentry 数据过载。

3.  **ELK Stack 用于日志收集与分析**：
    * 对于中大型应用或需要集中式日志管理、分析和可视化能力的场景，ELK 是非常强大的选择。
    * **推荐使用 Filebeat 收集应用日志文件**，这是最可靠和侵入性最小的方式。
    * Logstash 用于复杂日志格式的解析、转换和丰富。
    * Kibana 提供强大的可视化能力，帮助你发现趋势、监控指标和进行故障排查。

**整体流程建议：**

```
用户请求
   |
Web 框架 (Django/FastAPI/Flask)
   |
   +--- Python `logging` 模块 ---> 本地日志文件 (/var/log/my_app/app.log)
   |                                     |
   |                                   Filebeat
   |                                     |
   +--- 未捕获异常 / 手动捕获异常 ---> Sentry SDK (通过 DSN) ---> Sentry 服务 (实时告警/错误聚合)
   |                                     |
   |                                     |
   V                                     V
应用响应                            Logstash (可选，日志处理) ---> Elasticsearch (存储/索引) ---> Kibana (可视化/分析)
```

通过这三者的结合，你将拥有一个全面、高效、可靠的日志系统，能够极大地提升应用的可观测性和稳定性。

 