# 用于方便查看回顾
# http://securitytech.cc/ 的免费文本教程

# [官网](securitytech.cc) 会长期更新工具、产品开发、产品开发教程、商业产品开发。视频教程。

# [本人介绍](http://securitytech.cc/about)

![公众号](https://github.com/haidragon/haidragon/blob/main/gzh.png)


 ---

## 定时任务与异步队列实践：Celery / APScheduler

在 Web 开发中，我们经常会遇到一些耗时操作，例如发送邮件、生成报表、处理大量数据或执行计划任务。如果这些操作直接在主请求线程中执行，会严重阻塞用户请求，导致响应延迟甚至超时，极大地损害用户体验。

为了解决这个问题，我们需要引入**定时任务**和**异步队列**。

* **定时任务**：用于在未来的某个特定时间或按周期性地执行任务，例如每天凌晨生成日报、每周一发送营销邮件。
* **异步队列**：用于将耗时任务从主应用线程中剥离，放入一个队列中。然后，由单独的**工作进程 (Worker)** 从队列中取出任务并异步执行，从而立即释放主请求线程，提高应用的响应速度和并发能力。

本实践将主要围绕两个最流行的 Python 异步任务框架：**Celery** 和 **APScheduler**，并结合常见的消息中间件。

---

### 一、核心概念与消息中间件

在异步任务系统中，有几个核心角色：

1.  **生产者 (Producer)**：你的 Web 应用（Django/FastAPI/Flask）扮演生产者角色。当需要执行耗时任务时，它不是直接执行，而是将任务的描述（任务名称、参数）发送到消息中间件。
2.  **消息中间件 (Message Broker)**：也称为消息队列。它负责接收生产者发送的任务，并将其存储起来，等待工作进程来获取。它解耦了生产者和消费者。常见的消息中间件有：
    * **Redis**：轻量级，性能高，适合中小规模应用。可以用作 Celery 或 RQ 的消息代理。
    * **RabbitMQ**：功能强大，可靠性高，支持多种消息协议，适合大型、复杂的分布式系统。是 Celery 最推荐的消息代理之一。
    * **Kafka**：高吞吐量分布式消息系统，常用于大数据场景，不直接作为传统任务队列的消息代理，但可以作为事件流。
3.  **消费者/工作进程 (Consumer/Worker)**：单独运行的进程。它们连接到消息中间件，不断地从队列中拉取任务，然后执行这些任务。
4.  **结果后端 (Result Backend)**：可选组件。工作进程在完成任务后，可以将任务的执行结果或状态存储到这里，供生产者查询。Redis 或数据库（如 PostgreSQL）常被用作结果后端。

---

### 二、方案一：Celery (分布式任务队列)

**Celery** 是一个功能丰富、高度可扩展的分布式任务队列系统，它支持多种消息中间件和结果后端。它是处理异步任务和定时任务的首选方案，尤其适合中大型应用。

#### 1. Celery 特点

* **分布式**：可以部署多个工作进程在不同的机器上。
* **高可用**：通过消息中间件的持久化和工作进程的容错机制。
* **灵活性**：支持多种消息中间件 (Redis, RabbitMQ, SQS 等) 和结果后端 (Redis, Database, Memcached 等)。
* **定时任务**：内置 `Celery Beat` 组件用于管理和执行周期性任务。
* **丰富的任务特性**：重试机制、任务分组 (group)、链式调用 (chain) 等。

#### 2. Celery 基础配置

**准备工作：**

1.  **安装 Celery**：`pip install celery`
2.  **安装消息中间件客户端**：
    * **Redis**：`pip install "celery[redis]"` 或 `pip install redis`
    * **RabbitMQ**：`pip install "celery[librabbitmq]"` 或 `pip install pika`
3.  **安装结果后端客户端** (如果需要存储结果)：
    * **Redis**：同上。
    * **数据库**：例如 `pip install psycopg2-binary` (PostgreSQL)。

**Celery 配置 (`proj/celery.py`)**

创建一个 Celery 应用实例，通常放在项目根目录下的一个 `celery.py` 文件中：

```python
# proj/celery.py (或者 apps/celery.py)
import os
from celery import Celery

# 设置 Django 的 settings 模块 (如果你的项目是 Django)
# os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'your_django_project.settings')

# 创建 Celery 应用实例
# broker: 消息中间件的地址
# backend: 结果后端的地址 (可选)
app = Celery('your_project_name',
             broker='redis://localhost:6379/0', # 使用 Redis 作为 broker
             backend='redis://localhost:6379/1', # 使用 Redis 的不同数据库作为 backend
             include=['your_project_name.tasks']) # 告诉 Celery 在哪里找到任务

# Celery 配置
app.conf.update(
    task_track_started=True, # 任务开始时记录状态
    task_ignore_result=False, # 不忽略任务结果，可以查询
    # 可选：时区设置，确保定时任务按预期执行
    timezone='Asia/Shanghai', # 或者 'UTC'，'Asia/Shanghai' 等
    enable_utc=True, # 是否使用 UTC 时间
)

# 如果是 Django 项目，自动发现任务
# app.autodiscover_tasks()
# 如果是 Flask/FastAPI 项目，需要手动导入或指定 include 参数
```

**定义任务 (`proj/tasks.py`)**

在一个单独的文件中定义你的任务函数。

```python
# proj/tasks.py
from proj.celery import app
import time

@app.task
def add(x, y):
    print(f"Executing add task: {x} + {y}")
    time.sleep(2) # 模拟耗时操作
    result = x + y
    print(f"Add task finished: {result}")
    return result

@app.task
def send_email_task(recipient, subject, body):
    print(f"Sending email to {recipient} with subject '{subject}'...")
    time.sleep(5) # 模拟发送邮件耗时
    print(f"Email sent to {recipient}.")
    return {"status": "success", "recipient": recipient}
```

#### 3. 运行 Celery

1.  **启动 Redis 或 RabbitMQ 服务**。
2.  **启动 Celery Worker**：
    ```bash
    # 在你的项目根目录 (包含 proj/celery.py 的上一级目录) 运行
    celery -A proj.celery worker --loglevel=info # 启动一个 worker 进程
    # 或者对于 Windows 用户，可能需要加上 -P solo 或 -P gevent
    # celery -A proj.celery worker --loglevel=info -P solo
    ```
3.  **启动 Celery Beat (用于定时任务)**：
    ```bash
    # 在你的项目根目录运行
    celery -A proj.celery beat --loglevel=info
    ```
    * 注意：Celery Beat 通常只需要一个实例运行。

#### 4. 在框架中调用 Celery 任务

##### a. Django 集成 Celery

1.  **项目结构**：
    ```
    my_django_project/
    ├── manage.py
    ├── my_django_project/
    │   ├── __init__.py
    │   ├── settings.py
    │   ├── urls.py
    │   ├── wsgi.py
    │   └── celery.py # Celery 应用实例
    ├── my_app/
    │   ├── tasks.py # 定义 Celery 任务
    │   ├── views.py
    │   └── ...
    ```
2.  **`my_django_project/__init__.py`**：确保 Celery 应用被加载。
    ```python
    # my_django_project/__init__.py
    from .celery import app as celery_app

    __all__ = ('celery_app',)
    ```
3.  **`my_app/tasks.py`**：定义你的任务，如上面的 `add` 和 `send_email_task`。
4.  **在 Django 视图中调用任务**：
    ```python
    # my_app/views.py
    from django.http import JsonResponse
    from .tasks import add, send_email_task

    def trigger_add_task(request):
        # 异步调用 add 任务
        # .delay() 是 .apply_async() 的简写形式
        task_result = add.delay(10, 20)
        return JsonResponse({"message": "Add task triggered!", "task_id": task_result.id})

    def trigger_email_task(request):
        recipient = request.GET.get('to', 'test@example.com')
        send_email_task.delay(recipient, "Test Subject", "Hello from Celery!")
        return JsonResponse({"message": f"Email task to {recipient} triggered!"})

    def check_task_status(request, task_id):
        # 查询任务状态
        from celery.result import AsyncResult
        result = AsyncResult(task_id, app=add.app) # 使用任务所属的 Celery app 实例
        return JsonResponse({
            "task_id": task_id,
            "status": result.status,
            "result": result.result if result.ready() else None # 只有任务完成才有结果
        })
    ```
5.  **配置定时任务 (Django settings.py)**：
    ```python
    # my_django_project/settings.py
    CELERY_BROKER_URL = 'redis://localhost:6379/0'
    CELERY_RESULT_BACKEND = 'redis://localhost:6379/1'
    CELERY_ACCEPT_CONTENT = ['json']
    CELERY_TASK_SERIALIZER = 'json'
    CELERY_RESULT_SERIALIZER = 'json'
    CELERY_TIMEZONE = 'Asia/Shanghai' # 与 celery.py 保持一致

    # Celery Beat 定时任务配置
    from celery.schedules import crontab

    CELERY_BEAT_SCHEDULE = {
        'add-every-minute': {
            'task': 'my_app.tasks.add', # 任务的绝对路径
            'schedule': crontab(minute='*'), # 每分钟执行一次
            'args': (5, 5),
        },
        'send-daily-report': {
            'task': 'my_app.tasks.send_email_task',
            'schedule': crontab(hour=0, minute=0), # 每天凌晨 00:00 执行
            'args': ('admin@example.com', 'Daily Report', 'Here is your daily report.'),
        },
    }
    ```

##### b. FastAPI 集成 Celery

1.  **项目结构**：
    ```
    my_fastapi_project/
    ├── main.py
    ├── celery_app.py # Celery 应用实例
    ├── tasks.py # 定义 Celery 任务
    ```
2.  **`celery_app.py`**：同上 `proj/celery.py`，但不需要 `DJANGO_SETTINGS_MODULE`。
    ```python
    # celery_app.py
    from celery import Celery
    app = Celery(
        'fastapi_tasks',
        broker='redis://localhost:6379/0',
        backend='redis://localhost:6379/1',
        include=['tasks'] # 明确包含 tasks 模块
    )
    app.conf.update(
        task_track_started=True,
        task_ignore_result=False,
        timezone='Asia/Shanghai',
        enable_utc=True,
    )
    ```
3.  **`tasks.py`**：同上 `proj/tasks.py`，确保从 `celery_app` 导入 `app`。
4.  **`main.py`**：
    ```python
    # main.py
    from fastapi import FastAPI, BackgroundTasks, HTTPException
    from celery.result import AsyncResult
    from tasks import add, send_email_task # 从 tasks.py 导入任务

    app = FastAPI(title="FastAPI Celery Demo")

    @app.post("/trigger_add/")
    async def trigger_add_api(x: int, y: int):
        task_result = add.delay(x, y) # 异步调用 add 任务
        return {"message": "Add task triggered!", "task_id": task_result.id}

    @app.post("/trigger_email/")
    async def trigger_email_api(recipient: str, subject: str = "Test Subject", body: str = "Hello from FastAPI!"):
        send_email_task.delay(recipient, subject, body) # 异步调用发送邮件任务
        return {"message": f"Email task to {recipient} triggered!"}

    @app.get("/task_status/{task_id}")
    async def get_task_status(task_id: str):
        result = AsyncResult(task_id, app=add.app) # 同样需要 Celery app 实例
        if not result:
            raise HTTPException(status_code=404, detail="Task not found.")
        return {
            "task_id": task_id,
            "status": result.status,
            "result": result.result if result.ready() else None
        }

    # FastAPI 自身的 BackgroundTasks 适用于简单的后台任务，
    # 但不适合分布式、持久化或定时任务
    @app.post("/background_task/")
    async def use_background_tasks(background_tasks: BackgroundTasks):
        def write_log(message: str):
            with open("log.txt", "a") as f:
                f.write(message + "\n")
            time.sleep(1) # 模拟一点延迟
        background_tasks.add_task(write_log, "This is a simple background log.")
        return {"message": "Background task added (non-Celery)."}
    ```

##### c. Flask 集成 Celery

1.  **项目结构**：
    ```
    my_flask_project/
    ├── app.py
    ├── celery_app.py # Celery 应用实例
    ├── tasks.py # 定义 Celery 任务
    ```
2.  **`celery_app.py`**：同上 `fastapi_tasks/celery_app.py`。
3.  **`tasks.py`**：同上 `fastapi_tasks/tasks.py`。
4.  **`app.py`**：
    ```python
    # app.py
    from flask import Flask, jsonify, request
    from celery_app import app as celery_app # 导入 Celery app 实例
    from tasks import add, send_email_task # 从 tasks.py 导入任务
    from celery.result import AsyncResult

    app = Flask(__name__)

    @app.route("/")
    def index():
        return "Flask Celery Demo. Try /trigger_add?x=10&y=20 or /trigger_email?to=test@example.com"

    @app.route("/trigger_add")
    def trigger_add_task_flask():
        x = int(request.args.get('x', 1))
        y = int(request.args.get('y', 1))
        task_result = add.delay(x, y)
        return jsonify({"message": "Add task triggered!", "task_id": task_result.id})

    @app.route("/trigger_email")
    def trigger_email_task_flask():
        recipient = request.args.get('to', 'default@example.com')
        send_email_task.delay(recipient, "Flask Test Subject", "Hello from Flask Celery!")
        return jsonify({"message": f"Email task to {recipient} triggered!"})

    @app.route("/task_status/<task_id>")
    def get_task_status_flask(task_id: str):
        result = AsyncResult(task_id, app=celery_app)
        if not result:
            return jsonify({"error": "Task not found."}), 404
        return jsonify({
            "task_id": task_id,
            "status": result.status,
            "result": result.result if result.ready() else None
        })

    if __name__ == "__main__":
        app.run(debug=True, port=5000)
    ```

---

### 三、方案二：APScheduler (轻量级任务调度器)

**APScheduler (Advanced Python Scheduler)** 是一个用于安排 Python 代码执行的库。它更侧重于**调度 (Scheduling)**，而不是分布式任务队列。它可以在应用程序内部直接运行，无需单独的消息中间件和工作进程（尽管它也支持多种 Job Store 来持久化任务）。

#### 1. APScheduler 特点

* **轻量级**：可以直接集成到你的 Python 应用中，无需额外的消息中间件（但可以配置）。
* **调度类型多样**：
    * `date`：在某个特定日期时间执行一次。
    * `interval`：每隔一段时间执行。
    * `cron`：类似于 Linux 的 Cron 表达式，用于复杂的周期性调度。
* **持久化**：支持多种 Job Store (Memory, SQLAlchemy, MongoDB, Redis, RethinkDB, Zookeeper) 来持久化调度信息，防止应用重启后任务丢失。
* **执行器 (Executors)**：支持同步 (sync)、线程池 (threadpool)、进程池 (processpool) 等执行器。

#### 2. APScheduler 基础使用

**准备工作：**

1.  **安装 APScheduler**：`pip install APScheduler`
2.  如果需要持久化任务，还需要安装相应的 Job Store 依赖，例如 `pip install SQLAlchemy` (用于数据库)。

**示例：基本调度**

```python
# scheduler_demo.py
from apscheduler.schedulers.blocking import BlockingScheduler # 阻塞式调度器，适合脚本
from apscheduler.schedulers.background import BackgroundScheduler # 后台调度器，适合Web应用
import datetime
import time

def my_job(text):
    print(f"[{datetime.datetime.now()}] Job executed: {text}")

# 1. 使用 BlockingScheduler (适合独立脚本或一次性演示)
# scheduler_blocking = BlockingScheduler()
# scheduler_blocking.add_job(my_job, 'date', run_date='2025-06-21 10:00:00', args=['One-time job at 10:00'])
# scheduler_blocking.add_job(my_job, 'interval', seconds=5, args=['Interval job every 5 seconds'])
# scheduler_blocking.add_job(my_job, 'cron', hour=1, minute=30, args=['Daily 1:30 AM job'])
# scheduler_blocking.start()
# print("Blocking Scheduler started...")

# 2. 使用 BackgroundScheduler (适合 Web 应用集成)
scheduler_background = BackgroundScheduler()
scheduler_background.start() # 启动调度器

# 添加任务
scheduler_background.add_job(my_job, 'interval', seconds=3, args=['Interval job in background'])
scheduler_background.add_job(my_job, 'cron', second='*/10', args=['Cron job every 10 seconds'])

print("Background Scheduler started. Press Ctrl+C to exit.")
try:
    # 保持主线程运行，否则后台调度器会随主线程退出而停止
    while True:
        time.sleep(1)
except (KeyboardInterrupt, SystemExit):
    scheduler_background.shutdown()
    print("Scheduler shut down.")
```

#### 3. 在框架中集成 APScheduler

##### a. Django 集成 APScheduler

Django 项目通常推荐使用 `django-apscheduler` 库，它提供了更好的集成和管理界面。

1.  **安装 `django-apscheduler`**：`pip install django-apscheduler`
2.  **`settings.py`**：
    ```python
    # settings.py
    INSTALLED_APPS = [
        # ...
        'django_apscheduler', # 添加到 INSTALLED_APPS
    ]

    # APScheduler 配置 (可选，使用默认配置通常足以开始)
    # APSCHEDULER_DATETIME_FORMAT = "N j, Y, f:s a"
    # APSCHEDULER_RUN_NOW_TIMEOUT = 25 # seconds
    ```
3.  **`urls.py`**：
    ```python
    # urls.py (如果需要 APScheduler 的管理界面)
    from django.contrib import admin
    from django.urls import path, include

    urlpatterns = [
        path('admin/', admin.site.urls),
        path('django-rq/', include('django_rq.urls')), # 如果同时使用 RQ
        path('apscheduler/', include('django_apscheduler.urls')), # APScheduler 的管理界面
    ]
    ```
4.  **创建调度器启动脚本 (例如 `my_django_project/scheduler_starter.py`)**：
    由于 Django 应用通常由 Gunicorn 等 WSGI 服务器启动，我们需要确保 APScheduler 在应用启动时也启动。
    ```python
    # my_django_project/scheduler_starter.py
    # 这个文件通常不会直接运行，而是在你的 Django WSGI 文件中导入并调用
    # 或者通过一个 Django management command 来启动

    import os
    import django
    from apscheduler.schedulers.background import BackgroundScheduler
    from django_apscheduler.jobstores import DjangoJobStore
    from django_apscheduler.job_executions import DjangoJobExecution
    from apscheduler.triggers.cron import CronTrigger
    from django_apscheduler.models import DjangoJob
    import datetime

    # 设置 Django 环境
    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'my_django_project.settings')
    django.setup()

    # 定义你的任务函数
    def my_django_task():
        print(f"[{datetime.datetime.now()}] Django APScheduler task executed!")
        # 可以在这里导入你的 Django 模型，执行数据库操作等
        # from my_app.models import MyModel
        # MyModel.objects.create(name="Scheduled Item")

    def start_scheduler():
        scheduler = BackgroundScheduler()
        # 配置 Job Store，这里使用 Django 数据库作为持久化存储
        scheduler.add_jobstore(DjangoJobStore(), "default")

        # 检查并添加任务 (确保任务不重复添加)
        # 可以通过 Django admin 界面添加/管理任务
        # 或者在这里用代码添加
        try:
            # 示例：添加一个每 10 秒执行一次的任务
            scheduler.add_job(
                my_django_task,
                trigger=CronTrigger(second='*/10'),
                id='my_periodic_django_task',  # 任务 ID 必须唯一
                max_instances=1, # 确保只有一个实例运行
                replace_existing=True, # 如果存在同 ID 任务则替换
                jobstore='default'
            )
            print("Django APScheduler: Task 'my_periodic_django_task' added/replaced.")
        except Exception as e:
            print(f"Django APScheduler: Error adding task: {e}")

        # 启动调度器
        scheduler.start()
        print("Django APScheduler started.")

    # 如果在 manage.py runserver 启动时调用，可以这样
    # 注意：这会导致每个 runserver 进程都启动一个调度器，
    # 生产环境应该单独运行调度器，或者只在一个主进程中启动
    # if __name__ == '__main__':
    #     start_scheduler()
    #     try:
    #         while True:
    #             time.sleep(1) # Keep main thread alive
    #     except (KeyboardInterrupt, SystemExit):
    #         scheduler.shutdown()
    ```
    **注意**：在生产环境中，APScheduler 通常建议作为**单独的进程**运行，或者通过 `supervisord` 等工具确保只有一个 `APScheduler` 实例启动。如果把它放在每个 WSGI worker 中，可能会导致任务重复执行。`django-apscheduler` 提供了一个 `runjobs` 管理命令，可以作为单独的进程启动调度器：
    ```bash
    python manage.py runjobs # 运行一个调度器进程
    ```
    然后你可以在 Django admin 界面 (`/admin/django_apscheduler/djangojob/`) 管理和添加任务。

##### b. FastAPI / Flask 集成 APScheduler

FastAPI 和 Flask 可以直接在应用启动时初始化 `BackgroundScheduler`。

**`my_fastapi_project/main.py` (FastAPI)**

```python
from fastapi import FastAPI
from apscheduler.schedulers.background import BackgroundScheduler
import datetime
import uvicorn

app = FastAPI(title="FastAPI APScheduler Demo")
scheduler = BackgroundScheduler()

def fastapi_scheduled_task():
    print(f"[{datetime.datetime.now()}] FastAPI APScheduler task executed!")
    # 这里可以执行数据库操作、调用其他服务等
    pass

@app.on_event("startup")
async def startup_event():
    scheduler.start()
    # 添加一个每 5 秒执行一次的任务
    scheduler.add_job(
        fastapi_scheduled_task,
        'interval',
        seconds=5,
        id='fastapi_periodic_task',
        replace_existing=True
    )
    print("FastAPI: APScheduler started and task added.")

@app.on_event("shutdown")
async def shutdown_event():
    scheduler.shutdown()
    print("FastAPI: APScheduler shut down.")

@app.get("/")
async def read_root():
    return {"message": "FastAPI with APScheduler"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

**`my_flask_project/app.py` (Flask)**

```python
from flask import Flask, jsonify
from apscheduler.schedulers.background import BackgroundScheduler
import datetime
import atexit # 用于在应用关闭时清理调度器

app = Flask(__name__)
scheduler = BackgroundScheduler()

def flask_scheduled_task():
    print(f"[{datetime.datetime.now()}] Flask APScheduler task executed!")
    # 这里可以执行数据库操作、调用其他服务等
    pass

# 启动调度器
scheduler.start()

# 添加一个每 5 秒执行一次的任务
scheduler.add_job(
    flask_scheduled_task,
    'interval',
    seconds=5,
    id='flask_periodic_task',
    replace_existing=True
)
print("Flask: APScheduler started and task added.")

# 在应用关闭时关闭调度器
atexit.register(lambda: scheduler.shutdown())

@app.route("/")
def index():
    return jsonify({"message": "Flask with APScheduler"})

if __name__ == "__main__":
    app.run(debug=True, port=5000)
```

---

### 四、Celery 与 APScheduler 的选择

| 特性           | Celery                                       | APScheduler                                  |
| :------------- | :------------------------------------------- | :------------------------------------------- |
| **定位** | **分布式**任务队列，异步任务处理首选         | **本地**任务调度器，适合轻量级、集成式调度   |
| **部署模式** | 需要单独启动 Worker 和 Beat 进程，依赖消息中间件 | 可直接集成到应用进程，或作为独立进程运行     |
| **并发能力** | 强，通过多 Worker 和分布式部署支持高并发       | 取决于执行器配置 (线程/进程池)，局限于单机   |
| **可靠性** | 高，通过消息代理持久化、任务重试、ACK 机制实现 | 取决于 Job Store 是否持久化，以及应用本身的稳定性 |
| **可扩展性** | 极强，可轻松横向扩展 Worker 数量             | 局限于单机纵向扩展，集群需要额外的协调机制   |
| **功能丰富性** | 任务重试、链式调用、任务分组、监控 (Flower) 等 | 定时调度、多种 Trigger、Job Store 持久化     |
| **消息中间件** | 强制依赖 (Redis, RabbitMQ 等)                | 可选，但通常无需，除非用于 Job Store         |
| **学习曲线** | 较陡峭，概念较多，部署配置相对复杂           | 较平缓，易于上手和集成                       |
| **适用场景** | 中大型分布式应用，需要处理大量后台任务、高并发、任务持久化、跨服务器调度 | 中小型应用，单机任务调度、周期性报表、数据清理等 |

**选择建议：**

* **如果你的应用规模较大，需要处理大量并发任务，或者未来有分布式部署的计划，以及需要高可靠性、复杂任务编排和监控，那么 Celery 是更专业的选择。**
* **如果你的应用规模较小，任务量不大，主要是在单个应用实例中进行周期性或延时任务调度，不想引入额外的消息中间件和复杂部署，那么 APScheduler 是一个简单有效的解决方案。**

在某些情况下，两者也可以结合使用：APScheduler 用于应用内部的轻量级调度，而 Celery 则用于更重型的、需要分布式处理的异步任务。